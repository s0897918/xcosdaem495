GPUs play an important role in the computing capabilities of modern microprocessors. Therefore, they have an increasing significance in the future. For example, with the fast advancement of image processing, the computing power demand has become increasingly large. Moreover, the growing energy efficiency has made more and more computing power feasible. As the number of cores per chip grows, the energy consumption of cores is increasing too. A high performance computing CPU usually utilizes dozens of billions of processing units, each of which has dozens of Tera floating-point (TFP) cycles per second
GPU Elapsed time: for bigscience/bloom-1b1 is  12.458894326002337
GPUs play an important role in future applications. The key idea is to use a microcontroller inside the GPUs to be able to control multiple devices. At the same time, the device will use the energy efficiency of GPUs and achieve better performance. The microcontroller inside GPUs is named GPU controller, and this will be the central core of future GPUs.
In this paper, the microcontroller inside the GPU is called GPU kernel. There are various kinds of microcontrollers inside GPUs. The most common is that the CPU and GPU are all
CPU Elapsed time: for bigscience/bloom-1b1 is  55.808118661050685
speedup for: token = 100, model = bigscience/bloom-1b1, speedup = 4.479379726704668
GPUs play an important role in the development of GPUs.
Typically, GPUs have up to 2 GBs of RAM installed which can be used for both local storage and remote storage in one model. Although there are different models using different amounts of RAM for the purpose of improving the performance, the RAM installed to date is generally in the order of 2 GBs. A few of the models in recent years include a GPU with a single 2 GB RAM and others have higher amounts even up to 4 GBs such as NVIDIA's Titan X GPU which currently holds 7 and more GB RAM in both systems.
As a result, there is a need in the art to provide improved storage technologies for storing information in the future which will be used in future computing systems. It is desired that future computing systems provide improved storage technologies to allow improved data caching, which is especially important for online use of computing systems. A significant amount of computing resources may be occupied for the past a number of weeks to months due to the large amounts of work being performed by the system
GPU Elapsed time: for bigscience/bloom-1b1 is  12.00682713999413
GPUs play an important role in computer vision systems. Each GPU consists of a large number of CPUs and other components that are linked together. For example, a GPU in image processing applications is usually used to process images. A GPU accelerates a given computation by taking advantage of the locality, parallelism, and concurrency among CPUs.
One type of GPU accelerator is a parallel processor accelerator. A parallel processor accelerator is typically provided for parallel processing by a number of CPUs. The CPUs compute independently, without a feedback mechanism between each other, and they are connected using an interconnect. When processing data, the CPUs may be divided into multiple compute units, each with its own CPU processor and a different set of storage units. For example, data may be processed using three CPUs in the same compute unit. For processing time, CPUs may be grouped into CPU pools, which are used to calculate and manage data by using various resources.
In general, multiple CPUs are employed in a parallel processor accelerator, and at
CPU Elapsed time: for bigscience/bloom-1b1 is  95.92681533598807
speedup for: token = 200, model = bigscience/bloom-1b1, speedup = 7.989355907062302
GPUs play an important role in our system which can improve the performance of both DCMD and DSC.
V. CONCLUSION
In this paper, we proposed an efficient data distribution scheme called DSD for computing the differential secant (DCMD or DSC) and a new efficient and practical GPU for this task. As the name implies, DSD is used to distribute an input data stream into two output streams one of which is a reference stream and the other one, the second output stream, includes a set of approximations to the input data set. The main objective of DSD is to find the nearest approximation for the input data without using any other data distribution method. We also showed how to use GPUs to significantly improve this process. Compared with the algorithms used to date, our DSD is more efficient in terms of computational time and space requirement. We evaluated the performance of our GPU and DSD on a benchmark that has been widely used to compare multiple algorithms over a GPU, the CV. We found that our DSD outperformed and nearly halved the benchmark time compared to previous versions of the benchmark. The results show that our GPU will be a useful tool to accelerate the execution of DCMD and DSC algorithms. Furthermore, our GPU approach can speed up the execution of algorithms with a high memory consumption such as DSC on GPUs. Future research into the efficient use of GPUs for DSC will be an interesting direction in our studies. 
| MATERIALS AND METHODS

A. THE DSP
GPU Elapsed time: for bigscience/bloom-1b1 is  13.36384895502124
GPUs play an important role in recent years [8, 9]. In addition, the number of video streams is growing every year [6].
A. Overview
The video compression standard JPEG2000 is an adaptation of the JPEG (Joint Photographic Experts Group) standard [2], and it takes advantage of the fact that data are represented by multiple image planes (RGB and YUV), which makes the compression of the same image much faster and more efficient than single image plane. However, the image can get compressed with multiple planes, although the compression ratio of the compressed image is slightly different than that of the original image. In particular, a single JPEG image will contain one image row and one image column, which is also called a line profile [2] ; therefore, a line profile image is a 2D array of pixels. Therefore, a compressed line profile image is a 2D array of line profiles.
In order to obtain a line profile image, a camera needs to first select a frame sequence by setting a trigger value for each frame, and it is necessary to convert it into line profiles by selecting pixels from each line profile. The whole frame is converted to line profiles by selecting pixels repeatedly from all the lines, and then the pixels along several lines are overlapped to get a complete line profile of a selected frame. In the literature, there are several different methods for constructing a line profile.
1) A line profile image is obtained by selecting a set of pixels from the lines and connecting them with their corresponding segments for each line profile. The pixel connectivity is shown in Figure 1.
CPU Elapsed time: for bigscience/bloom-1b1 is  135.48723561596125
speedup for: token = 300, model = bigscience/bloom-1b1, speedup = 10.13833934160519
GPUs play an important role in 5G systems, particularly in terms of improving the throughput, energy, and flexibility of systems. However, the implementation of a 5G SIM on each device is challenging, especially in the context of heterogeneous designs. As a result, many studies have been conducted for optimizing the performance of 3G and 4G/5G networks using different algorithms [8] [9] [10] [11] [12]. However, these algorithms usually focus on maximizing the performance of particular devices and do not integrate the computational aspects of 5G systems. For example, in [13], the optimal energy consumption was calculated using a probabilistic model to design the antenna patterns in a 5G communication system. In [14], the optimal power allocation was performed on a homogeneous network using an iterative algorithm. However, most of these studies focus on the energy consumption and lack a design framework combining the computation and energy performance aspects. The traditional energy estimation methodologies typically require the knowledge of the network topology to estimate the energy consumption for any given node or group of nodes. Therefore, the traditional energy models are insufficient in the context of the heterogeneous 5G system. Moreover, there remains a lack of research in the efficient and accurate energy estimation in 5G systems [15] [16] [17] [18]. Recently, Li et al. presented a framework of energy-aware applications in 5G. This framework aims at optimizing the energy efficiency of 5G subsystems by integrating the energy consumption with the spatial information. To achieve this goal, four key components are considered: a model of the local energy and power level, a model of the network performance, a model of the data forwarding processes, and a model of the computational processes. The power consumption is calculated through Equation (1). Meanwhile, the energy consumption is estimated using Equation (2) and the computational load is calculated using Equation (3).
where e is the energy consumption, r i and p i are the radio load, radio power level for i th user, and k is the number of users transmitting in the network. Therefore, e i and p i are the energy and power of user i.
where r
GPU Elapsed time: for bigscience/bloom-1b1 is  15.171634401078336
GPUs play an important role in the future computing field, and are also playing an important role in the computing field, and are also playing an important role in the internet of things (IoT) world. They are already widely used in military, education, healthcare, manufacturing, agriculture, and security. They also have an incredible application power in robotics, computer vision, data analytics, and many more. We need more hardware resources to handle all the applications that will be done in these new devices. It is therefore important to enhance the memory capacity of each chip to increase their capability to run a variety of applications.
As a new generation of device, the memory capacity is becoming a critical factor to determine the success of the new device. There are several memory configurations, and therefore, we need to consider several factors, such as the number of bits that needs to be stored in memory units, the speed at which memory data is being stored, and the amount of data that needs to be stored in memory units. To minimize power consumption of each device, it is important to store data at lower speeds and for a longer time. Some researchers have already shown that improving the processor frequency will reduce data communication time and energy consumption [2], which is used to improve the data transfer speed of the device.
In this paper, we investigate how the processor frequency affects the throughput of an FPGA embedded device. We study the power consumption of the processor because the power consumption of a processor is directly related to the computational speed [5]. Therefore, we want to design a processor with high performance to deal with applications that will be used in the future computing systems. The use of high-performance processors will reduce power consumption because it will be a power-saving device.
We begin by discussing the different types of processors. We then present the different configurations that are available to increase the processor speed. We then present how a processor is designed to improve the performance of the chip. Finally, we show how to analyze the performance of a processor by analyzing its energy consumption and data transfer speed
CPU Elapsed time: for bigscience/bloom-1b1 is  182.80030671099667
speedup for: token = 400, model = bigscience/bloom-1b1, speedup = 12.048820969348167
GPUs play an important role in GPUs today as well (Nakagawa et al., 2007). It is possible that in the future, when a system is more powerful there may be a significant improvement in the use of GPUs, as shown in (Becker et al., 2011). With the advent of high-performance NVIDIA GPUs, in the next phase in the life line of CPSI, the number of processing units may also increase and the storage requirements will be much larger. In this phase, the number of storage units will also increase, so the number of users is also likely to increase. In particular, we anticipate that in the period between the two phases described above, cloud environments can be expected to have an even greater impact on NVIDIA GPUs. In this scenario, users will increasingly use them for data analysis in real-time and to save data. Additionally, these users may start to make use of some of the other computing resources in order to obtain the best results.
Finally, NVIDIA GPUs may benefit from the evolution of the computing platforms. In the future, they will become more powerful, which will also enable the user to run different applications in parallel, i.e. to use many GPUs on the same system in order to complete a larger set of tasks within less time than the previous configuration. Furthermore, NVIDIA GPUs will be able to work at multiple processing levels, i.e. they will be able to communicate among multiple units at the same time.
CONCLUSIONS
The performance demands of CPSI applications in the near future will increase due to both the increasing number of users and the growing area of application areas to be processed. Several research projects and initiatives have been proposed to address this growing demand. These are summarized as follows:
• Integration between NVIDIA and NVIDIA Workspace
• Collaboration of GPUs with NVIDIA GPUs
• Multi-GPUs • Clustering and Grid Computing. Each of these has some advantages and disadvantages. The aim of this paper is to evaluate the application of NVIDIA GPUs to real-world situations, as it is currently difficult to conduct such evaluations, especially due to the number of challenges and issues that must be taken into account. Additionally, the research challenges differ in the types of CPSI applications studied. This paper presents, for the first time, an analysis of the application of GPUs to real applications involving cloud computing. NVIDIA works on GPUs to produce real-time 3D graphics graphics cards. Additionally, in this paper we describe some
GPU Elapsed time: for bigscience/bloom-1b1 is  16.131229290971532
GPUs play an important role in today's embedded devices, and GPUs are becoming increasingly integrated in mobile devices. They provide additional resources to support the computing process that the device needs in order to interact with external applications. On the hardware side, GPUs include GPUs that have an integrated core processor or cores that are connected through the main memory to one or more peripheral devices (GPUs can also be called GPU-provided processors). The CPU of most modern devices includes powerful and large cores that provide up to 1.6 GFlops of processing power, which can be used to accomplish many tasks. These CPUs are very powerful and use a number of resources, including the main memory (a type of storage with which applications communicate with the CPU (memory units)). Since GPUs are embedded on the main memory of modern devices, the major part of the computational resources of a GPU are located in that memory.
It is important to understand that GPUs are not only used in high-end devices, but they are also often used in lower-cost devices. For example, as a consumer device, the AMD Radeon HD 4290 GPU is an example of a device. Here, the GPU has to communicate with the main memory to obtain processing capability. The Radeon HD 4290 is also an example of an integrated GPU device. Integrated GPU devices have a single CPU embedded on a single substrate, and provide much higher performance than traditional computer-based chips, such as CPUs. Integrated GPU device designers generally rely upon GPUs to implement tasks that are performed by a CPU. Using the integrated GPU, designers typically do not need to worry about the location of the CPU within the GPU and the processing power of the CPU.
In order to support the computing of high-performance applications on integrated GPU devices, the integrated GPU device designers tend to use a large number of different hardware devices, each of which can perform one or more computing tasks. For example, a CPU may use more than one set of registers on an integrated GPU device. In addition, CPUs and GPUs may communicate with each other through the same or different interfaces, such as ports (e.g., PCI). In general, integrated GPU device designers use at least three different GPUs to perform a typical computation. A CPU and a GPU in an integrated GPU device may work together. It is often the case that the CPU and a GPU are embedded on separate substrates. However, when combined with a CPU, the number of hardware devices that can support a computation may be further increased.
Many
CPU Elapsed time: for bigscience/bloom-1b1 is  228.39315995399375
speedup for: token = 500, model = bigscience/bloom-1b1, speedup = 14.158447309519234
GPUs play an important role in the mobile computing market, with the proliferation of Wi-Fi and other networking technology making the wireless device available on all mobile platforms. Thus, in order for mobile computing to be an integral part of our daily lives there will be a need to meet this development in terms of mobile devices that are capable of running the appropriate operating-system. As this is true for mobile devices as a whole, this will also be true of mobile computing products. Mobile computing devices can be divided into desktop computing devices, i.e. portable computers with a built-in display, and mobile computing devices, i.e. handheld computing (micro, pocket and compact devices). The first category comprises mobile devices that are highly portable and can be easily carried around and are used in the fields of professional work and educational application. The second type, however, comprises mobile devices that are not readily portable and that are used on a more leisurely basis. As a rule, in the latter type of mobile computing devices there is a need for a higher level of user control and a higher degree of user interface accessibility. This is especially true for portable devices. One example of this type of mobile computing product, with the user interface having a greater degree of accessibility, is the personal digital assistant (PDA), or mobile workstation. Personal digital assistants are equipped with one or more of a keyboard, a mouse, and a display that can be used for information entry. Although the functionality of those devices may be adequate for normal communication in the home environment, the more sophisticated features of a personal digital assistant may require the use of additional equipment, which can substantially increase the price of this kind of device. The second category comprises mobile devices that are not readily portable and are used only as a tool or for personal communication. Mobile phones, for example, are one example of this type of mobile device.
There is also the need to provide technology suitable for supporting and making use of mobile computing devices. The most fundamental requirement, when considering mobile computing devices, is that these devices have a large user-interface for the user. A great deal is still being learned in this area as mobile computing devices continue to advance. A mobile computing product that meets some of these requirements will be of use to both the office and home user, especially in the case of portable computers with a built-in display, but these mobile computing devices also need to have a highly effective and user-friendly user interface. The user interface for mobile computing devices should support the main functions of the device as rapidly as possible, should be easy to understand and operate, and should be attractive for the user. Users should be able to be guided by the mobile device in the use thereof and should be able to easily apply the new advanced features without having to redefine the user interface design and application.
The present invention has the object of providing a simple mobile computing device which is easy to use. A basic feature of the invention is the provision of a user interface in the form of a user terminal. This user terminal includes a combination of a keyboard
GPU Elapsed time: for bigscience/bloom-1b1 is  17.523441627970897
GPUs play an important role in high-throughput bioinformatics analyses and medical imaging. However, most existing GPUbased high-throughput data analyses require massive computational resources (NVIDIA Corporation, 2015). For example, there are more than 50,000 different protein-protein interactions (PPI). These high-throughput data are analyzed by high-throughput pipelines called pipelines. The major bottleneck has been GPUs. Given that the computational power and memory capacity of GPUs are still significantly larger than those of CPUs, pipelines still can not process data in high-throughput. In general, these pipelines do not fit well in GPUs, because the pipeline runs slower and takes longer to process a large number of interactions ( Figure 4 ). GPU-based high-throughput bioinformatics pipelines do not scale automatically, due to the fact that large computational workloads require high computational power on GPU devices, and the computational power is not completely distributed to GPU devices when using many processing units simultaneously. Even if such a data bottleneck is taken into account, several studies have pointed out that even with the best CPU-based pipelines, they will not fit well in GPUs (Chiu et al.,, 2017 Xu et al., 2016).
Many researchers try to implement pipelines using modern CPU-based pipelines, but most are not effective and even unsatisfactory. This has led to the development of GPU-based parallel and distributed software engineering tools. There has been a trend that the use of GPUs in biology is no more on the increase as many scientific libraries and applications will not continue to use CPU-based pipelines. It is expected that these applications will replace or complement computational resources in GPU-based pipelines or even start adopting them. The development of such tools will help scientists achieve a greater degree of scientific data analysis and accelerate scientific experiments, which will eventually lead to a reduction in data storage costs associated with high-throughput analysis. Moreover, because of the use of GPUs to gather large amounts of data at different rates, a parallel computing paradigm will emerge.
In general, the major problem of high-throughput data-driven bioinformatics is the bottleneck of GPUs. Therefore, it is impossible to design computational resources to fit well in GPUs. In some cases, data-driven bioinformatics pipelines need to run in a multi-CPU environment, and in such situations, a parallel computing framework (in which each CPU is divided into several computational areas) is used. Using a unified platform has proved to be the efficient way to solve the problem.
Parallel computing can also be used to solve bottlenecks of high-throughput data-driven bioinformatics pipelines. Generally, the implementation of parallel computing requires a large number of computational resources, including the computing time and memory capacity of the CPU and GPU. As stated above, a parallel and distributed computing framework will provide a large amount of computational resources, but these resources are not completely distributed on GPU devices. In general, data-driven bioinformatics pipelines will use the same framework in many applications,
CPU Elapsed time: for bigscience/bloom-1b1 is  271.1671005260432
speedup for: token = 600, model = bigscience/bloom-1b1, speedup = 15.474534414130533
GPUs play an important role in computing, and they are ubiquitous in computer systems in order to support many computing services. Although there is a variety of performance evaluation methods, a common use of the state-of-the-art methods usually focus on the static (static) power model in the computation network. For example, the conventional method of static power model based on energy model [15], is suitable for most of the practical use of power models. However, the traditional methods do not consider the dynamic energy supply model when modeling the power of computing system, thus lead the computer system to be inefficient in the current state. In order to make the static power model in the computing system match the power generation requirements, it is necessary to use the power generation model of power system that is also based on various energy models. 
Modeling and Simulation of Dynamic Power Generation of Computing System
To meet the energy supply demand of computing application, the design of energy system is essential. Based on the power models and various energy models. We developed Model-based Energy-Supply Model (MEMS) which can capture the energy flows between the power grid, computing system, sensor nodes, and energy storage devices. The proposed MEMS is based on the energy models and the power systems. In this section, we give the technical details about the method and the details of model.
Model-Based Energy-Supply Model (MEMS)
For computer system, the computing application can access storage devices directly on the power grid through the power switch. In this case, the energy supply sources are always stored in the grid.
On the other hand, if a sensor needs to be remotely operated by communication system, the information of energy supply of device with computing application is required. Therefore, the data of such information should be acquired periodically from the grid to the computing system. Although in the current computer system, there are many communication systems which support the communication. Currently, most of the communication systems are realized by the Zigbee communication system [19]. Most of the embedded sensors have the capability of transmitting and receiving data by the Zigbee communication system, such as the heart rate sensors, strain gauge sensor, and so forth [20]. Nowadays, the development of Zigbee communication system progresses rapidly. According to the status of research, the current high availability, high speed, and low energy consumption Zigbee communication system has become the dominant communication system of the world. We propose MEMS model according to the Zigbee communication system model (ZCCSM), which is realized by the ZCCSM protocol standard [19]. According to the status of research, the current high availability, high speed, and low energy consumption Zigbee communication system has become the dominant communication system of the world. The ZCCSM protocol structure includes data processing and transmission modules. The data processing unit is used to store energy consumption information and energy storage information and to determine which modules to generate new energy at different times. In the communication system, the power generation module can be realized by a power transformer. To enhance the accuracy of power transformation, the main transformer should be designed in a modular way. To overcome the disadvantages associated with the use of power transformers, a low cost power supply module in a modular way should be integrated into the power supply unit [21]. Furthermore, two batteries are often used in the power supply modules, and a small circuit with several devices is used to generate the required amount of power in the Zigbee communication system. To reduce the power supply cost, each of the batteries is usually connected with a small circuit that produces
GPU Elapsed time: for bigscience/bloom-1b1 is  19.133047887007706
GPUs play an important role in the computation speedup of some Nvidia GPUs. They are also known to work efficiently for the memory access and synchronization problem in the GPU implementation. Recently, it has been established that the performance of the GPU can be greatly improved for the task-oriented computation [4]. Therefore, the authors of the article have designed a novel algorithm for the GPU scheduling and parallelized communication, which is discussed as follows.
The rest of the article is organized as follows. In section 2, we present our algorithm for the parallelized communication. In section 3, we present the experiments and analysis. Finally, we conclude the article in section 4.
Parallelized Communication
The parallelized communication algorithm (PCA) consists of two components: parallelization part and synchronization part. In order to perform the parallelized communication, we firstly divide the communication into two types, i.e. global synchronization and task synchronization. After the synchronization, the execution of the parallelization is completed. The global synchronization can be divided into two parts, i.e. the global synchronization of data storage and the global synchronization of the GPU controller. In Fig. 1, PCA starts by computing a global synchronization in the global data storage. The data are stored into global data array and the global data array must be synchronized with the global data storage if the communication among the CPU and GPU is to be performed. This synchronization process is realized by checking the synchronization table of the global data array which contains addresses and data type of the global data array which are used for the execution of the global synchronization or for computing the total communication. Once the global synchronization is computed, the communication of data array is divided into the two types, i.e. global synchronization and task synchronization. Then PCA performs the task synchronization and global synchronization of GPU data. The data array synchronization at the CPU level follows the same protocol with the data array synchronization at the GPU level.
The synchronization data array consists of the number of the global data array, the number of the task data array and the number of the task data array which are used to compute the global data array synchronization or to compute the total communication. After the synchronization, when the task synchronization is computed in PCA, the data array synchronization is divided into task synchronization and global synchronization. The task synchronization is determined by the task synchronization of the global data array in PCA. The global data array synchronization is determined by the global data synchronization of the global data array in the global storage. The data array synchronization in the global storage and the GPU controller are divided into two processes in PCA. The data array synchronization at the CPU level follows one of the two processes. The synchronization data array at the GPU controller is processed by the second process. During the synchronization process, PCA performs parallel data array synchronizations on the global data array. We suppose the parallelization starts with the global data array synchronization. Before the global data array synchronization, the data array synchronization at the CPU level is determined by the global data synchronization at the CPU level. Once the global data array synchronization at the CPU level is computed, the global data array synchronization is divided into the second and third processes (i.e. task synchronization and global data array synchronization) for the execution of task synchronization. The data array synchronization at the GPU controller is divided into different processes and is implemented by the third process. After the data array synchronization computation is completed in the global storage, the data array synchronization is divided into the data synchronization and global synchronization which follows the second and third processes to perform global synchronization or
CPU Elapsed time: for bigscience/bloom-1b1 is  350.47092583891936
speedup for: token = 700, model = bigscience/bloom-1b1, speedup = 18.317569051656772
GPUs play an important role in enhancing the general computing capacity of the system which can be further enhanced by using a dual-core processor. Hence it is essential to optimize the energy performance of such systems.
We have presented an analytical framework of optimization that addresses the optimization of energy consumption while maintaining a given system architectural constraints. Such an optimization can be efficiently implemented by iterative solver which has been shown to reduce the computational effort. Further we have presented a new method to address this problem for dual-core processors in terms of energy performance optimization, which has been shown to reduce the computational time. Further, we have presented an efficient numerical tool for implementation of such a dual-core power balance method. The method was implemented in C++ and was shown to have comparable energy efficiency with the commercial solver which can be run on any platform. The simulation is performed on the Intel Xeon Phi processors to demonstrate the efficiency of the proposed method. 
ACKNOWLEDGEMENT
This work has been part of the NSF grant IIS 0225373.
APPENDIX A NUMERICAL CONVERGENCE ANALYSIS
This section presents an asymptotic analysis of the performance of the proposed technique for dual-core systems. Since the proposed method takes a very long time (approaching the performance of linearized Eulerian simulation), in the computation of the total energy consumption we consider time-steps of O(nT 0 ) where T 0 is the time at which first grid level is generated by the code and n is the total number of grid points.
Thus, in this problem at T 0 the total energy consumption is given by
where E 0 = 1 − r − is the normalized energy which measures the energy rate of energy consumption;
t is the energy spent in generating grid points at time timestep t and e 0 is the total energy spent in the initialization phase. In which, ρ = 1 2 is the ratio of the rate of power dissipation over the rate of power dissipation in generating the grid function;
is the energy efficiency of the scheme and f is the total energy cost;
is the energy efficiency of the solution approach and f is the total energy cost. As can be seen from equations (4) and (6), the total energy consumption has quadratic term. Thus to get a better performance for the system we have to increase the resolution of our analysis. The following theorem provides an asymptotic analysis of the performance of the proposed method for dual-core processors.
Theorem 2 At the equilibrium the total energy consumption of
is the solution approach. Also, f is the energy efficiency of the solution approach such that f = E 0 − r − ; E 0 is the energy efficiency of the first step of the Newton-Raphson solver used in the optimization. Then for a given solution approach, E 0 ≤ ǫE 0 ≤ ǫ 2 f where
Proof: By using the definitions given in (4) the following asymptotic relation is valid for any ǫ > 0
In which, (8) and (9) and taking ǫ → 0 and α → 0 we obtain the following asymptotic relation
Thus, choosing α → 0 and ǫ → 0 we obtain
Then, using equations (2) and (3), (10) becomes
Moreover, using integration by parts and taking ǫ → 0 and α → 0 we get
Therefore, by equation (11), for ǫ → 0 the convergence rate of the iterative method is given by
where
is the energy efficiency of the first step of the Newton-Raphson solver used in the optimization.
Further, since r − ≤ 1/2, the last two terms (with α → 0 and ǫ → 0) of (12) is less than 1/2. Thus, by taking ǫ → 0 and α → 0 it can be concluded that for ǫ → 0 the convergence rate of the proposed method is given by
Using the above stability condition and equation (15), the energy of the first step of the Newton-Raphson solver used in the optimization is given by
Thus, the asymptotic stability condition is satisfied and the total
GPU Elapsed time: for bigscience/bloom-1b1 is  20.853981487918645
GPUs play an important role in enhancing computational efficiency for online video processing tasks, such as search for matches [7], filtering out noise and motion artefacts [8], motion tracking [9], and detection of keypoints in videos [10].
To meet the demanding needs of online video processing, modern GPUs enable computation of operations on data sets that represent millions of pixels. Many algorithms designed in the past decades are computationally demanding in order to keep the complexity to a maximum. For example, searching a large number of pixels by brute-force methods does not allow to store all the pixels in memory. Furthermore, the number of iterations or the number of generations needed to reach a certain accuracy also increases quite a lot [11]. This is particularly true if one starts with an initial solution and uses GPU-accelerated versions of existing algorithms. Hence, it is obvious that the best method to run algorithms on GPU is to parallelize them [12]. In this way one can achieve a higher degree of parallelization and better scalability in case the required computational time is critical, and it is a necessary prerequisite for any application. For example, in the context of computer vision one has to find a matching result that takes as much time as possible, even if the exact solution is known. It is obvious from this perspective that a GPU-accelerated version of one of the current most popular algorithms for matching online, namely, the GCM [13] will play an important role for speeding up matching tasks by taking advantage of parallelization. In this paper we demonstrate this using the GCM as the benchmark algorithm. We compare the performance of the algorithm against implementations of the GCM where different parts of the code were taken of different algorithms from the state-of-the-art.
Parallelization of matching algorithms plays an important role in many computer vision applications. Although a number of existing approaches have been proposed, the algorithms used for parallelization usually do not represent an optimal solution for all tasks. As we showed in Section 3, the GCM algorithm is particularly well suited for parallelization. Although it represents the best-known matching algorithm for online video processing, it can consume a significant amount of CPU time to achieve reasonable accuracy. The need of such a parallelization has been recognized for several decades and many existing implementations have been proposed. However, there is still a huge demand for high-fidelity implementations with better efficiency and smaller computation cost. In this paper we demonstrate that the algorithm is not well suited for any of those limitations and we propose a new strategy on the basis of which the GCM can be parallelized. Furthermore, we show that this can lead to significant reductions in both compute and memory usage. Due to its popularity, the benchmark algorithm can provide a good starting point for new studies.
The paper is organized as follows. In Section 2 we briefly introduce the GCM algorithm. In Section 3, we present the benchmark algorithms used for our experiments. We compare the GPU implementations of GCM and other existing algorithms from the state-of-the-art. Finally, in Section 4 we present the conclusions.
GCM EXPERIMENTS
Recently, numerous GPU implementations of the GCM algorithm have been proposed, but all of them do not include any measure of performance for online applications. Although the GCM algorithm is known to be very accurate on most of the real-world images, it does not show significant time savings when compared to a GPU implementation where the CPU time is taken into account when calculating the similarity score between two feature vectors. One of the biggest problems regarding these implementations is that the implementation typically contains several loops and functions for calculating pixel similarity and therefore the implementation is rather complex. It is also important to note that the GCM implementation used in this paper, i.e., the one presented in [13], is based on a specific benchmark algorithm developed from scratch by the authors. The algorithm is a very large program that takes as many CPU cycles as possible to calculate pixel similarity. Due to the high processing power of the GPU the whole computation can be carried out in a few weeks to months. Unfortunately, the benchmark
CPU Elapsed time: for bigscience/bloom-1b1 is  366.37400202895515
speedup for: token = 800, model = bigscience/bloom-1b1, speedup = 17.568539717041894
